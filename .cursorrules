# Instructions

During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Check the System
Before you start using the tools, you should chech the which system you are using. The following examples are written in unix syntax, if you are using windows, you need to convert the syntax to windows syntax.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {host|openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="host",  # or "anthropic" or "openai"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.
- When using QMT or miniQMT API terminal, all strategy files must include "#coding:gbk" as the first line to ensure proper encoding compatibility with the QMT platform. But except for the case you are writing code for the QMT API terminal, most of the time, you should use utf-8 encoding.
- All database models should inherit from both Base and TimestampMixin to ensure consistent timestamp tracking (created_at and updated_at) across all tables. These fields are essential for data lineage, audit trails, and incremental data updates. We've verified this requirement with check_schema.py and documented the database structure in docs/database_schema.md. The TimestampMixin is defined in backend/data/models/base.py and automatically adds created_at and updated_at columns to any model that inherits from it. We've also created a comprehensive schema validation tool (check_schema.py) that can verify all tables adhere to this requirement.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use proper style names like 'whitegrid', 'darkgrid', etc. instead of deprecated 'seaborn-v0_8-whitegrid' styles
- Use 'gpt-4o' as the model name for OpenAI's GPT-4 with vision capabilities
- When writing code, always add debug information to stderr to help you debug the code. You should have a clear plan before you start writing the code. For example, you should print proper logs to know what structure you get if your code is fetching table structurized data.
- When writing modules, always write tests for functions with logical connections and include integration tests to ensure they work smoothly together.
- When working with Chinese financial APIs (like East Money, Sina Finance), implement retry mechanisms with exponential backoff and multiple data source fallbacks due to potential connection instability and rate limits.
- When using AKShare for financial data, always check multiple endpoints for the same data type (e.g., financial statements have both em and sina sources) and implement proper column name mapping as different sources may use different Chinese characters for the same metrics.
- When using AKShare APIs, carefully check the required stock code format for each function as they may differ: some require pure code (600519), some need market suffix (600519.SH), and others need market prefix (SH600519). Always document the required format in function docstrings.
- When designing database schemas for Chinese stock data, store stock codes in granular components (stock_number, market_code) in addition to the combined format to handle different API requirements and enable flexible querying. For example, store "600519" (stock_number), "SH" (market_code), and "600519.SH" (stock_code) separately.
- When implementing signal generation and risk management systems for Chinese A-shares:
  - Use lot size of 100 shares for position calculations
  - Consider sector-specific exposure limits
  - Implement correlation-based risk metrics
  - Track northbound flow for institutional activity
- When working with financial data collectors, ensure standardize_numeric returns Decimal objects instead of floats for precision in financial calculations
- When mocking financial API responses for tests, ensure the mock data structure exactly matches the actual API response format including all column names and data types
- When implementing financial data processors, be careful with imports and ensure all dependencies are correctly specified to avoid import errors
- When designing RESTful APIs for financial systems, use proper HTTP status codes (e.g., 404 for not found, 401 for unauthorized, 400 for bad request) and include detailed error messages in the response
- When implementing authentication for APIs, use JWT tokens with appropriate expiration times and secure storage mechanisms
- When creating FastAPI applications, organize code into routers for better maintainability and separation of concerns
- When implementing API endpoints, use Pydantic models for request/response validation and documentation
- When working with financial data in APIs, ensure proper handling of decimal values by using Decimal type instead of float
- When setting up Cypress tests, create custom commands for common actions like authentication and data stubbing
- When testing React components with charts, mock the charting library to avoid SVG rendering issues
- When implementing WebSocket services, include auto-reconnection logic with exponential backoff
- For frontend production deployment, use multi-stage Docker builds to minimize image size and Nginx for serving the static files
- When writing tests and debugging, always refer with Docs/td_standard.md.
- Never skip tests for critical business functionality like data retrieval. Instead, fix the underlying issues by consulting API documentation (e.g., akshare.md), implementing proper error handling, and using fallback data sources. Data retrieval is a core function that must be reliable and well-tested—implement multiple data source fallbacks to ensure data availability.
- For financial data collection, always implement a multi-source fallback strategy. When implementing collectors for market data like funds flow, examine the akshare documentation carefully to identify the correct API functions. For example, northbound fund flow data can be retrieved using `stock_hsgt_hist_em`, `stock_hsgt_fund_flow_summary_em`, or `stock_hsgt_individual_em` with different parameters. Implement a cascading fallback strategy where if the primary data source fails, the system tries alternative sources automatically.
- When writing API mocks for testing, ensure you're mocking the correct API path. For imported modules (like 'import akshare as ak'), patch the actual import path used in the code (e.g., 'backend.data.collectors.funds_flow.ak.stock_hsgt_hist_em') rather than the original module path ('akshare.stock_hsgt_hist_em').
- When dealing with Chinese financial data, pay attention to unit conversions. Chinese financial data is often reported in units like "亿" (100 million) or "万" (10,000), so ensure proper scaling when storing in the database.
- Ensure all collectors properly handle date ranges for filtering data, and thoroughly test this functionality with appropriate test cases that verify the parameters are correctly passed to the underlying APIs.
- For incremental tables like daily_quotes, it's critical to properly test incremental update functionality including correct handling of date ranges, updating existing records, and validating data. Never skip tests for these critical components as they ensure data consistency over time.
- When testing collectors of time-series data, ensure proper validation of high/low price relationships, negative values, and other data quality rules to maintain data integrity.
- Use the appropriate data types for financial data: pandas Int64 for volumes, float/Decimal for prices, and properly formatted dates for timestamps.
- For collector classes, properly mock class methods by using patch.object with the class (collector.__class__) instead of the instance (collector) to ensure the mock works correctly within the class instance.
- When implementing command line arguments in data collection scripts, use appropriate types (e.g., use float instead of int for --years parameter to support values like 0.5) to ensure flexibility and usability.
- When collecting financial data for Chinese A-shares stocks, specifically handle special markets like Science and Technology Innovation Board (688/689 prefix), ChiNext (300 prefix), and B shares (900 prefix) as these stocks often have limited or different financial data available. Implement graceful degradation that allows processing to continue even when data is unavailable for certain stocks.
- For financial data collectors, implement a multi-tiered fallback strategy with at least three different data sources (e.g., Sina, Financial Abstract API, East Money) to maximize data availability. Use robust column mapping to handle differences in naming conventions across sources.
- Always return empty collections (empty DataFrame, empty list) instead of raising exceptions when no data is available from financial APIs to avoid pipeline interruptions. Log detailed errors but allow processing to continue for other stocks.
- In large data collection operations, implement transaction management at the individual stock level rather than the entire batch to prevent large transaction failures and ensure partial success is captured properly.
- When implementing financial data collectors, follow the proper development process: first consult the API documentation (e.g., akshare.md), then create comprehensive tests, and only after tests pass should you run the actual data collection code. This ensures robust and reliable data collection.
- When working with East Money financial APIs, pay special attention to stock code formatting - some APIs require market prefix format (SH600519) rather than market suffix format (600519.SH). Always test both formats if API documentation is unclear.
- For Chinese financial data APIs, implement smart column detection that can handle variations in column naming across different API versions and data sources. Look for key terms like "净资产收益率" (ROE), "每股收益" (EPS), etc. in column names rather than expecting exact matches.
- When handling Chinese date formats (e.g., "2023年12月31日"), implement proper parsing logic to convert them to standard formats for consistency across your application.
- Structure error handling in financial data collectors to ensure that errors in one API call don't prevent trying alternative data sources. Use flags or state variables to control the fallback flow rather than relying solely on try-except blocks.
- When implementing data collection scripts like expand_data_collection.py, ensure the --index all option correctly fetches all A-shares stocks using StockInfoCollector.fetch_stock_list() method, which provides a comprehensive list of stocks from the market. This is essential for complete market analysis and backtesting.
- For data collection scripts, use float type for time period parameters (like --years) to support fractional values (e.g., 0.5 years = 6 months), providing more flexibility in historical data range selection.
- For batch processing financial data, never abort the entire process when a single stock fails - log the error, increment a counter for reporting purposes, and continue with the next stock to ensure maximum data coverage, especially for funds flow and other potentially unstable data sources.
- Always handle missing or unexpected data structures in API responses by checking for column existence before accessing them, and have fallback strategies for when primary data sources fail or return incomplete data.
- When working with Chinese stock markets, don't forget to support the Beijing Stock Exchange (BJ) in addition to Shanghai (SH) and Shenzhen (SZ). Stock codes typically follow patterns: Shanghai starts with '6', Shenzhen with '0' or '3', and Beijing with '4' or '8', which should be used for proper market code identification.
- For performance-critical tasks like financial data collection, implement multi-threading with thread-local storage for database sessions to improve throughput while maintaining data consistency. When using a shared database between threads, ensure each thread has its own session to prevent thread safety issues.
- When optimizing API calls, consider adjusting API call order based on reliability observations, putting more reliable APIs first to reduce total API attempts and improve success rates.
- For APIs with frequent timeouts, use configurable timeout parameters to fail quickly rather than waiting for system defaults, especially in multi-threaded contexts where thread blocking can significantly impact overall performance.
- Always follow the proper development process: first consult API documentation, then create tests, and 
only after tests pass should you run the actual data collection code.
- Implement smart column detection to handle variations in column naming across different API versions 
and data sources.
- Structure error handling to ensure errors in one API call don't prevent trying alternative data sources 
- use flags or state variables to control the fallback flow.

- When managing multiple databases in a project, standardize on a single database for development and a single database for production to avoid data inconsistencies and maintenance overhead. Use environment variables to control which database is used in different environments.
- When creating database migration scripts, use batch processing with proper error handling to ensure data integrity during large transfers. Always log progress and validate data after migration.
- For database connection strings, use environment variables with sensible defaults to make the application deployable in different environments without code changes.
- When dropping databases, always implement safety checks and confirmation prompts to prevent accidental data loss. Connect to the 'postgres' database (not the one being dropped) to perform drop operations.

## Multi-threaded Database Operations and Data Collection

### SQLAlchemy Best Practices
- When working with SQLAlchemy in multi-threaded environments, use thread-local session management to prevent concurrency issues. Create a new session for each thread using a factory function and ensure proper session closure.
- Use SQLAlchemy's `query.update()` method for batch updates instead of directly modifying model attributes. This is more efficient and avoids race conditions in concurrent environments.
- When checking for record existence and then updating, use the `one_or_none()` method for clearer code and to avoid multiple database round-trips.
- For model attribute assignments involving SQL expressions, use appropriate SQLAlchemy constructs like `case` statements or `func` methods instead of Python conditionals.
- Never compare SQLAlchemy model attributes directly with Python operators like `==` when building queries; use the proper SQLAlchemy operators and functions instead.

### Database Transaction Management
- Implement nested transaction contexts in batch operations to allow partial successes - wrap individual record processing in subtransactions so failures in one record don't roll back the entire batch.
- Use explicit transaction boundaries with `session.begin()`, `session.commit()`, and `session.rollback()` in complex operations rather than relying on SQLAlchemy's implicit transaction management.
- For long-running batch operations, commit in smaller chunks (e.g., every 100-500 records) to reduce lock contention and memory consumption.
- When handling existing records, use SQLAlchemy's `merge()` method with care as it can be inefficient for large datasets - prefer explicit querying and updating for better performance.
- Include explicit error handling around transaction boundaries to ensure proper cleanup in case of exceptions, especially in multi-threaded contexts.

### Multi-threaded Data Collection Patterns
- Design data collection systems with a cascading fallback strategy - try the most reliable data source first, then fall back to alternative sources if the primary fails, all within the same processing cycle.
- Use thread-local storage for maintaining state within worker threads, especially for database connections and session objects that aren't thread-safe.
- Implement proper progress tracking in multi-threaded environments using thread-safe counters or queues, combined with libraries like `tqdm` for user-friendly progress display.
- Structure multi-threaded data collection with a thread pool executor pattern where the main thread coordinates and monitors, while worker threads perform the actual data collection and processing.
- Centralize statistics collection in a thread-safe manner, using atomic counters or locks to prevent race conditions when updating success/failure counts.

### Error Handling Strategies
- Never abort an entire batch process due to errors in individual records - use proper exception handling to log errors, mark failures, and continue processing the remaining items.
- Implement contextual error logging that captures the full context of failures (e.g., which stock, which API, which parameters) to facilitate debugging and monitoring.
- Design data collectors to degrade gracefully - when primary APIs fail, fall back to alternative sources automatically while maintaining data consistency.
- Use appropriate retry mechanisms with exponential backoff for network-related failures, but implement circuit breakers to prevent overwhelming unstable services.
- Structure error handling as a pipeline where each stage can recover from failures in previous stages, using a combination of alternatives, fallbacks, and defaults.

### Testing and Validation
- Create comprehensive test fixtures that precisely match the structure of external API responses, including edge cases like missing fields, null values, and unexpected data types.
- Implement data validation at multiple levels: input parameters, API responses, processed data, and database records to ensure data integrity throughout the pipeline.
- Use integration tests that verify the entire data flow from collection to storage, confirming that proper error handling, fallbacks, and transaction management work correctly.
- For financial data, implement domain-specific validations that check for logical consistency (e.g., high price >= low price, open and close within high/low range).
- Test multi-threaded code specifically for race conditions and transaction isolation by designing test cases that intentionally introduce conflicts or errors in concurrent operations.
- Each test file should focus on one module/component
- Use descriptive test names that explain the scenario
- Include both positive and negative test cases
- Mock external dependencies appropriately
- Add proper error handling tests
- Document test setup and requirements

### Performance Optimization
- Batch similar operations together to reduce database round-trips - collect multiple records before committing them in a single transaction.
- Use SQLAlchemy's bulk operations (`bulk_insert_mappings`, `bulk_update_mappings`) for significant performance improvements when working with large datasets.
- Tune thread pool sizes based on the nature of the workload - I/O-bound tasks (like API calls) benefit from higher concurrency than CPU-bound tasks.
- Implement connection pooling with appropriate timeout and overflow settings to handle surge loads without exhausting database connections.
- Monitor and log operation timing to identify bottlenecks in the data collection and processing pipeline, focusing optimization efforts on the slowest components.

## Debugging Best Practices

### Systematic Problem-Solving Approach
- Always follow a structured debugging methodology: identify the problem, understand the expected behavior, reproduce consistently, isolate the issue, analyze root cause, implement fix, and verify solution.
- When dealing with data processing issues, prioritize understanding the data flow from source to final storage before attempting fixes.
- Create dedicated debug scripts in the appropriate test directories to isolate and reproduce issues in a controlled environment.
- When debugging data collectors, verify API call parameters, response handling, data transformation, and storage steps individually.

### DataFrame Operations Best Practices
- Always use `df.copy()` when creating a new DataFrame from an existing one to avoid unintended reference issues.
- For numeric data in financial applications, use `Decimal` objects rather than floats to preserve precision.
- When creating a new DataFrame, always ensure it contains all required columns and appropriate defaults for missing values.
- When comparing Decimal values in tests, convert to float or use approximate equality checks to avoid decimal precision comparison issues.

### Code Structure and Data Flow
- Understand the full method call chain before modifying any single function - changes in one function can have ripple effects.
- Pay special attention to data type conversions between API responses, DataFrame processing, and database models.
- Place test files in the proper directory structure that mirrors the source code organization.
- Create debug scripts that provide detailed logging of internal state at critical processing points.

# Scratchpad

# Database Consolidation Plan [COMPLETED]

## Current Database Situation
After analysis, I've found three databases with the "qmt" prefix:

1. **qmt** - Main production database with 5,396 stock records and all required tables
   - Tables: daily_quotes, finance_data, funds_flow, signals, stock_info
   - This appears to be the primary database in active use

2. **qmt_general** - Development/test database with only 3 stock records
   - Tables: alembic_version, daily_quote, daily_quotes, finance_data, funds_flow, signals, stock_info
   - Has both "daily_quote" and "daily_quotes" tables (possible schema inconsistency)
   - Has alembic migrations set up

3. **qmt_mvp** - Empty database with no tables
   - Appears to be unused

## Database Configuration Files
Multiple files reference different database URLs:

1. **backend/api/database.py**:
   - Updated to use environment variable: `DATABASE_URL`
   - Default: `postgresql://postgres:postgres@localhost:5432/qmt_general`

2. **backend/data/models/base.py**:
   - Updated to use environment variable: `DATABASE_URL`
   - Default: `postgresql://localhost/qmt_general`

3. **.env**:
   - Current: `postgresql://marvi:@localhost:5432/qmt`

4. **alembic.ini**:
   - Updated to use a placeholder value
   - Actual value loaded from environment in env.py

5. **alembic/env.py**:
   - Updated to load DATABASE_URL from environment
   - Default: `postgresql://localhost/qmt_general`

6. **docker-compose.yml**:
   - Set to: `postgresql://postgres:postgres@db:5432/qmt`

## Consolidation Plan

### Step 1: Standardize on Two Databases [COMPLETED]
1. ✅ **Production**: `qmt` - Already contains the most data
2. ✅ **Development**: `qmt_general` - Already has Alembic migrations set up

### Step 2: Update Configuration Files [COMPLETED]
1. ✅ Update all default database URLs to use environment variables
2. ✅ Ensure .env and .env.example have clear documentation for both databases
3. ✅ Update alembic.ini to use environment variables instead of hardcoded values

### Step 3: Migrate Schema and Data [COMPLETED]
1. ✅ Use Alembic to ensure both databases have identical schema
2. ✅ Create a data migration script to copy missing data if needed

### Step 4: Clean Up [COMPLETED]
1. ✅ Create script to drop the unused `qmt_mvp` database
2. ✅ Create script to remove duplicate tables (e.g., daily_quote vs daily_quotes)

### Step 5: Documentation [COMPLETED]
1. ✅ Document the database structure and relationships
2. ✅ Update README with clear instructions on database setup and switching between environments

## Implementation Tasks
[X] Update backend/api/database.py to use consistent environment variables
[X] Update backend/data/models/base.py to use the same environment variables
[X] Update alembic.ini to use environment variables
[X] Update alembic/env.py to load environment variables
[X] Create Alembic migration to standardize schema across databases
[X] Create data migration script (scripts/migrate_database_data.py)
[X] Create script to drop unused database (scripts/drop_unused_database.py)
[X] Update documentation in README.md
[X] Create a shell script to automate the consolidation process (scripts/complete_database_consolidation.sh)

## Final Result
We have successfully consolidated the database configuration to use:
1. **qmt** for production
2. **qmt_general** for development

All configuration files now use the `DATABASE_URL` environment variable, which can be set in the `.env` file to switch between databases. We've created scripts to:
- Migrate data between databases
- Clean up duplicate tables
- Drop unused databases
- Verify database consistency

The README.md has been updated with clear instructions on database setup and management.

## Quantitative Trading System Development Plan

## Overview
Building a comprehensive quantitative trading system with stable and aggressive layers, focusing on data collection, strategy execution, and risk management.

## Project Structure
```
qmt_general/
├── backend/
│   ├── data/
│   │   ├── collectors/       # Data collection from AKShare
│   │   ├── processors/      # Data standardization & processing
│   │   └── models/         # Database models
│   ├── strategies/
│   │   ├── stable/         # Stable layer strategies
│   │   └── aggressive/     # Aggressive layer strategies
│   ├── api/               # API endpoints and authentication
│   │   ├── routers/       # API route handlers
│   │   └── auth.py        # Authentication module
│   └── risk/              # Risk management modules
├── frontend/
│   ├── components/        # React components
│   ├── pages/            # Page components
│   └── services/         # Frontend services
├── tests/
│   ├── backend/
│   │   ├── unit/
│   │   └── integration/
│   ├── frontend/
│   │   ├── unit/
│   │   └── integration/
│   └── e2e/
└── docs/                 # Project documentation
```

## Development Phases

### Phase 1: Environment Setup & Data Infrastructure [COMPLETED]
[X] Setup development environment
  [X] Create Python virtual environment
  [X] Initialize project structure
  [X] Setup database (PostgreSQL)
  [X] Configure development tools (linters, formatters)

[X] Create requirements files
  [X] Backend requirements.txt with all necessary dependencies
  [X] Frontend package.json dependencies (to be implemented)

[X] Database setup
  [X] Create database models based on er.md
  [X] Implement SQLAlchemy models
  [X] Write database migration scripts
  [X] Unit tests for database models
  [X] Integration tests for database operations
  [X] Fix relationship definitions and cascade behavior
  [X] Verify all tests are passing

[X] Data Collection Framework
  [X] Create base collector with common functionality
    [X] Retry mechanism with exponential backoff
    [X] Logging setup
    [X] Stock code format handling
    [X] Response validation
  [X] Implement stock info collector
    [X] Basic stock information collection
    [X] Data standardization
    [X] Unit tests
  [X] Implement daily quote collector
    [X] Daily stock data collection
    [X] Data standardization
    [X] Date range filtering
    [X] Unit tests
  [X] Implement finance data collector
    [X] Financial data collection
    [X] Data standardization
    [X] Report period filtering
    [X] Null value handling
    [X] Unit tests
    [X] Fix AKShare API parameter mismatch
    [X] Add standardize_numeric method
    [X] Fix column name mismatches
    [X] Fix test data and failures
  [X] Implement funds flow collector
    [X] Capital flow data collection
    [X] Data standardization
    [X] Date range filtering
    [X] Null value handling
    [X] Unit tests
  [X] Create data standardization layer
    [X] Stock code format standardization
    [X] Date format standardization
    [X] Numeric value standardization
    [X] Chinese text standardization
    [X] Unit tests
  [X] Integration tests for data pipeline
    [X] Basic data flow tests
    [X] Error handling and retry tests
    [X] Data consistency tests
    [X] Data standardization tests
    [X] Advanced integration tests
      [X] Data validation and cleaning
      [X] Concurrent database operations
      [X] Pipeline recovery
      [X] Time-series data consistency
      [X] Edge cases handling

### Phase 2: Backend Core Features [COMPLETED]
[X] Data Processing Modules
  [X] Plan implementation of data processors
  [X] Implement financial data processors
    [X] Create financial ratio calculator
    [X] Create growth rate calculator
    [X] Create valuation metrics calculator
    [X] Unit tests for financial processors
  [X] Create industry analysis module
    [X] Implement industry sector classification
    [X] Create industry performance comparison
    [X] Unit tests for industry analysis
  [X] Develop technical indicator calculator
    [X] Implement basic indicators (MA, MACD, RSI, etc.)
    [X] Create advanced indicators (Bollinger Bands, Ichimoku Cloud, etc.)
    [X] Unit tests for technical indicators

[X] Data Visualization Utilities
  [X] Implement chart generation functions
    [X] Create price chart generator
    [X] Create technical indicator chart generator
    [X] Create financial ratio chart generator
    [X] Create industry comparison chart generator
    [X] Create correlation heatmap generator
  [X] Create report generation functions
    [X] Implement stock report generator
    [X] Implement portfolio report generator
    [X] Implement industry report generator
    [X] Create report templates
  [X] Unit tests for visualization utilities
    [X] Chart generation tests
    [X] Report generation tests
    [X] Template validation tests

[X] Strategy Implementation
  [X] Stable layer strategy framework
    [X] Define strategy interface
    [X] Create position management classes
    [X] Implement backtest engine
    [X] Implement MA Crossover strategy
    [X] Unit tests for stable strategies
  [X] Aggressive layer strategy framework
    [X] Implement Mean Reversion strategy
    [X] Unit tests for aggressive strategies
  [X] Risk management module
    [X] Implement position sizing controls
    [X] Add stop loss and take profit functionality
    [X] Implement equity curve tracking
    [X] Unit tests for risk management

[X] API Development
  [X] Design RESTful API endpoints
    [X] Create API directory structure
    [X] Implement FastAPI application
    [X] Create authentication module
  [X] Implement authentication
    [X] Create JWT token-based authentication
    [X] Implement user management
  [X] Create API routers
    [X] Implement data router
    [X] Implement strategy router
    [X] Implement visualization router
  [X] Create API documentation
    [X] Document all endpoints
    [X] Create README.md with usage examples
  [X] API integration tests
    [X] Create test client
    [X] Implement basic endpoint tests

### Phase 3: Frontend Development [COMPLETED]
[X] Component Development
  [X] Create base Layout component with navigation
  [X] Create reusable StockChart component
  [X] Build form components
  [X] Implement notification system
  [X] Unit tests for components

[X] Page Implementation
  [X] Develop Login page
  [X] Create Dashboard page
  [X] Build Stock Analysis page
  [X] Create Strategy Backtest page
  [X] Implement Portfolio Monitor page
  [X] Create Settings page
  [X] Add API Documentation page

[X] State Management & Services
  [X] Implement authentication context
  [X] Create API service layer
  [X] Add WebSocket service for real-time updates
  [X] Unit tests for services

### Phase 4: Integration & Testing [IN PROGRESS]
[X] System Integration
  [X] Connect frontend with backend through API service
  [X] Implement real-time updates with WebSockets
  [X] Unit tests for integration components

[X] End-to-End Testing
  [X] Set up Cypress testing framework
  [X] Create test fixtures and mock data
  [X] Implement login/authentication tests
  [X] Add Dashboard page tests
  [X] Add Stock Analysis page tests
  [X] Create custom test commands

[X] Deployment Configuration
  [X] Create Dockerfile for frontend
  [X] Configure Nginx for serving frontend
  [X] Create Docker Compose for full-stack deployment
  [X] Add monitoring with Prometheus and Grafana

[ ] Documentation
  [X] Create frontend README
  [ ] Document API endpoints
  [ ] Create user guide
  [ ] Add developer documentation

### Phase 5: Deployment & Monitoring [Not Started]
[ ] Deployment Setup
  [ ] Configure production environment
  [ ] Create CI/CD pipelines
  [ ] Set up staging environment

[ ] System Monitoring
  [ ] Implement detailed logging
  [ ] Create monitoring dashboards
  [ ] Set up alerts

[ ] Performance Optimization
  [ ] Frontend bundle optimization
  [ ] API performance tuning
  [ ] Database query optimization

## Current Focus
We have completed Phase 3 (Frontend Development) and made significant progress on Phase 4 (Integration & Testing). We have successfully implemented:

1. Frontend Components and Pages:
   - Authentication system with Login page
   - Dashboard with portfolio metrics and charts
   - Stock Analysis with search and charting
   - Strategy Backtesting interface
   - Portfolio Monitor with holdings tracking
   - Settings page for user preferences
   - API Documentation page

2. Testing Infrastructure:
   - Unit tests for components with Jest and React Testing Library
   - End-to-end tests with Cypress
   - Custom test commands for common actions
   - Mock data fixtures

3. Deployment Configuration:
   - Multi-stage Dockerfile for frontend
   - Nginx configuration for serving the app
   - Full-stack Docker Compose setup
   - Monitoring with Prometheus and Grafana

Next, we need to complete the documentation and prepare for Phase 5 (Deployment & Monitoring).

## Next Tasks:
[ ] Complete API documentation
[ ] Create comprehensive user guide
[ ] Prepare developer documentation
[ ] Set up CI/CD pipelines
[ ] Configure production environment

## Testing Strategy
Each phase includes:
- Unit tests for individual components
- Integration tests for component interactions
- End-to-end testing for critical user flows
- Performance benchmarking

## Notes
- Ensure AKShare data standardization across different markets
- Maintain consistent error handling throughout the system
- Document all API endpoints and data structures
- Regular security audits for financial data handling
- Implement proper reconnection logic for WebSockets
- Use multi-stage Docker builds to optimize container size
- Configure proper CORS for API security
- Set up monitoring for system health and performance

### Next Task: Adding Industry Information for A-Shares

In accordance with business requirements in qtrader_manual.md, we need to ensure all A-shares have proper industry classification information, which is essential for:

1. Industry-based scoring (Section 3.1 in qtrader_manual.md)
2. Sector-based analysis of capital flows
3. Industry PE/PB historical percentile calculations
4. Sector rotation strategy implementation

#### Requirements
- All stocks must have industry classification data filled
- Use "申万一级/二级行业分类" as mentioned in the trader manual
- Implement proper validation and error handling
- Store information in the stock_info table's industry field
- Add incremental update capability to refresh industry data periodically

#### Implementation Plan
1. [X] Research AKShare APIs for industry classification data
2. [X] Extend StockInfoCollector to fetch and update industry information
3. [X] Add validation rules for industry data
4. [X] Implement tests according to testing standards
5. [X] Create a standalone utility to fill missing industry data for existing records

#### Potential AKShare APIs
- Based on the akshare documentation, we should investigate:
  - stock_sector_spot() - Provides current industry sector performance
  - stock_sector_detail() - Provides detailed sector classification
  - stock_individual_info_em() - Provides detailed stock info including industry

#### Technical Requirements
1. [X] Extend the StockInfoCollector.collect() and StockInfoCollector.incremental_update() methods to ensure industry data is included
2. [X] Add a fill_missing_industry() method to handle existing records without industry data
3. [X] Update tests to verify industry data is correctly retrieved and stored
4. [X] Add specific validation for industry data (e.g., not null, follows expected format)
5. [X] Document the industry classification scheme used in the codebase

#### Testing Strategy
Following td_standard.md guidelines:
1. [X] Create unit tests for industry data collection
2. [X] Test error handling and data validation
3. [X] Test with mock API responses
4. [X] Test the fill_missing_industry functionality
5. [X] Add integration tests for the industry update workflow

#### Implementation Summary
We have successfully implemented the industry information feature:

1. Enhanced the StockInfoCollector to fetch industry data from AKShare using the stock_individual_info_em API
2. Added methods to ensure all stocks have industry information:
   - fetch_industry_info: Gets industry data for a specific stock
   - ensure_industry_data: Fills in missing industry data in a DataFrame
   - fill_missing_industry_data: Updates existing database records with missing industry data
3. Updated the collect() and incremental_update() methods to ensure industry data is populated
4. Created a standalone utility script (tools/fill_industry_data.py) to fill in missing industry data for existing records
5. Added comprehensive tests for all new functionality

The implementation follows the testing standards in td_standard.md and meets the business requirements in qtrader_manual.md.

## Data Collector Fixes and Database Integration [COMPLETED]

### Issue Summary
We identified and fixed several critical issues with the `parallel_data_collection.py` script and associated collectors:

1. [X] Fixed SQLAlchemy attribute name mismatches in `DailyQuote` model
2. [X] Implemented proper database session management for concurrent operations
3. [X] Added robust error handling to prevent batch failures
4. [X] Fixed funds flow and finance data collection inconsistencies
5. [X] Enhanced fallback strategies for unstable data sources

### Implementation Summary
1. [X] Updated `collect_*_task` functions to use proper SQLAlchemy update methods
2. [X] Added thread-local session management for database operations
3. [X] Implemented proper transaction boundaries and error recovery
4. [X] Enhanced progress reporting with success/failure statistics
5. [X] Added comprehensive logging for debugging

### Test Results
Successfully tested all data collectors:
- Daily quotes collection: 242 records for 000001.SZ
- Funds flow collection: 1 record with proper fallback mechanisms
- Finance data collection: Proper skipping of recently updated records

All collectors now correctly save data to the database with proper error handling and session management.

## Financial Data Collection Best Practices

- When implementing financial data collectors, use a multi-source strategy with proper fallback mechanisms to ensure data availability and completeness.
- For Chinese financial data APIs:
  - Handle multiple column naming conventions (e.g., '净资产收益率', 'ROE', 'ROE(%)')
  - Properly handle percentage values by checking for values > 100
  - Use appropriate precision for different metrics (4 decimals for ROE/dividend_yield, 6 for EPS)
  - Consider special stock types (科创板、创业板) that may have limited data availability

- For data merging and standardization:
  - Always standardize dates to datetime format with proper error handling
  - Handle multiple date formats consistently
  - Use proper suffixes when merging data from different sources
  - Implement smart column detection for variations in naming

- For error handling and logging:
  - Log data completeness statistics for monitoring
  - Implement proper retry mechanisms with exponential backoff
  - Use detailed error messages that include context
  - Track success rates for different data sources

