# Instructions

During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Check the System
Before you start using the tools, you should chech the which system you are using. The following examples are written in unix syntax, if you are using windows, you need to convert the syntax to windows syntax.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {host|openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="host",  # or "anthropic" or "openai"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.
- When using QMT or miniQMT API terminal, all strategy files must include "#coding:gbk" as the first line to ensure proper encoding compatibility with the QMT platform. But except for the case you are writing code for the QMT API terminal, most of the time, you should use utf-8 encoding.
- All database models should inherit from both Base and TimestampMixin to ensure consistent timestamp tracking (created_at and updated_at) across all tables. These fields are essential for data lineage, audit trails, and incremental data updates. We've verified this requirement with check_schema.py and documented the database structure in docs/database_schema.md. The TimestampMixin is defined in backend/data/models/base.py and automatically adds created_at and updated_at columns to any model that inherits from it. We've also created a comprehensive schema validation tool (check_schema.py) that can verify all tables adhere to this requirement.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use proper style names like 'whitegrid', 'darkgrid', etc. instead of deprecated 'seaborn-v0_8-whitegrid' styles
- Use 'gpt-4o' as the model name for OpenAI's GPT-4 with vision capabilities
- When writing code, always add debug information to stderr to help you debug the code. You should have a clear plan before you start writing the code. For example, you should print proper logs to know what structure you get if your code is fetching table structurized data.
- When writing modules, always write tests for functions with logical connections and include integration tests to ensure they work smoothly together.
- When creating FastAPI applications, organize code into routers for better maintainability and separation of concerns
- When setting up Cypress tests, create custom commands for common actions like authentication and data stubbing
- When testing React components with charts, mock the charting library to avoid SVG rendering issues
- When implementing WebSocket services, include auto-reconnection logic with exponential backoff
- For frontend production deployment, use multi-stage Docker builds to minimize image size and Nginx for serving the static files
- When writing tests and debugging, always refer with Docs/td_standard.md.
- Never skip tests for critical business functionality like data retrieval. Instead, fix the underlying issues by consulting API documentation (e.g., akshare.md), implementing proper error handling, and using fallback data sources. Data retrieval is a core function that must be reliable and well-testedâ€”implement multiple data source fallbacks to ensure data availability.
- Ensure all collectors properly handle date ranges for filtering data, and thoroughly test this functionality with appropriate test cases that verify the parameters are correctly passed to the underlying APIs.
- For collector classes, properly mock class methods by using patch.object with the class (collector.__class__) instead of the instance (collector) to ensure the mock works correctly within the class instance.
- Always handle missing or unexpected data structures in API responses by checking for column existence before accessing them, and have fallback strategies for when primary data sources fail or return incomplete data.
- When optimizing API calls, consider adjusting API call order based on reliability observations, putting more reliable APIs first to reduce total API attempts and improve success rates.
- For APIs with frequent timeouts, use configurable timeout parameters to fail quickly rather than waiting for system defaults, especially in multi-threaded contexts where thread blocking can significantly impact overall performance.
- Always follow the proper development process: first consult API documentation, then create tests, and 
only after tests pass should you run the actual data collection code.
- Implement smart column detection to handle variations in column naming across different API versions 
and data sources.
- Structure error handling to ensure errors in one API call don't prevent trying alternative data sources 
- use flags or state variables to control the fallback flow.
- When managing multiple databases in a project, standardize on a single database for development and a single database for production to avoid data inconsistencies and maintenance overhead. Use environment variables to control which database is used in different environments.
- When creating database migration scripts, use batch processing with proper error handling to ensure data integrity during large transfers. Always log progress and validate data after migration.
- For database connection strings, use environment variables with sensible defaults to make the application deployable in different environments without code changes.
- When dropping databases, always implement safety checks and confirmation prompts to prevent accidental data loss. Connect to the 'postgres' database (not the one being dropped) to perform drop operations.

## Multi-threaded Database Operations and Data Collection

### SQLAlchemy Best Practices
- When working with SQLAlchemy in multi-threaded environments, use thread-local session management to prevent concurrency issues. Create a new session for each thread using a factory function and ensure proper session closure.
- Use SQLAlchemy's `query.update()` method for batch updates instead of directly modifying model attributes. This is more efficient and avoids race conditions in concurrent environments.
- When checking for record existence and then updating, use the `one_or_none()` method for clearer code and to avoid multiple database round-trips.
- For model attribute assignments involving SQL expressions, use appropriate SQLAlchemy constructs like `case` statements or `func` methods instead of Python conditionals.
- Never compare SQLAlchemy model attributes directly with Python operators like `==` when building queries; use the proper SQLAlchemy operators and functions instead.

### Database Transaction Management
- Implement nested transaction contexts in batch operations to allow partial successes - wrap individual record processing in subtransactions so failures in one record don't roll back the entire batch.
- Use explicit transaction boundaries with `session.begin()`, `session.commit()`, and `session.rollback()` in complex operations rather than relying on SQLAlchemy's implicit transaction management.
- For long-running batch operations, commit in smaller chunks (e.g., every 100-500 records) to reduce lock contention and memory consumption.
- When handling existing records, use SQLAlchemy's `merge()` method with care as it can be inefficient for large datasets - prefer explicit querying and updating for better performance.
- Include explicit error handling around transaction boundaries to ensure proper cleanup in case of exceptions, especially in multi-threaded contexts.

### Multi-threaded Data Collection Patterns
- Design data collection systems with a cascading fallback strategy - try the most reliable data source first, then fall back to alternative sources if the primary fails, all within the same processing cycle.
- Use thread-local storage for maintaining state within worker threads, especially for database connections and session objects that aren't thread-safe.
- Implement proper progress tracking in multi-threaded environments using thread-safe counters or queues, combined with libraries like `tqdm` for user-friendly progress display.
- Structure multi-threaded data collection with a thread pool executor pattern where the main thread coordinates and monitors, while worker threads perform the actual data collection and processing.
- Centralize statistics collection in a thread-safe manner, using atomic counters or locks to prevent race conditions when updating success/failure counts.

### Error Handling Strategies
- Never abort an entire batch process due to errors in individual records - use proper exception handling to log errors, mark failures, and continue processing the remaining items.
- Implement contextual error logging that captures the full context of failures (e.g., which stock, which API, which parameters) to facilitate debugging and monitoring.
- Design data collectors to degrade gracefully - when primary APIs fail, fall back to alternative sources automatically while maintaining data consistency.
- Use appropriate retry mechanisms with exponential backoff for network-related failures, but implement circuit breakers to prevent overwhelming unstable services.
- Structure error handling as a pipeline where each stage can recover from failures in previous stages, using a combination of alternatives, fallbacks, and defaults.

### Testing and Validation
- Create comprehensive test fixtures that precisely match the structure of external API responses, including edge cases like missing fields, null values, and unexpected data types.
- Implement data validation at multiple levels: input parameters, API responses, processed data, and database records to ensure data integrity throughout the pipeline.
- Use integration tests that verify the entire data flow from collection to storage, confirming that proper error handling, fallbacks, and transaction management work correctly.
- For financial data, implement domain-specific validations that check for logical consistency (e.g., high price >= low price, open and close within high/low range).
- Test multi-threaded code specifically for race conditions and transaction isolation by designing test cases that intentionally introduce conflicts or errors in concurrent operations.
- Each test file should focus on one module/component
- Use descriptive test names that explain the scenario
- Include both positive and negative test cases
- Mock external dependencies appropriately
- Add proper error handling tests
- Document test setup and requirements

### Performance Optimization
- Batch similar operations together to reduce database round-trips - collect multiple records before committing them in a single transaction.
- Use SQLAlchemy's bulk operations (`bulk_insert_mappings`, `bulk_update_mappings`) for significant performance improvements when working with large datasets.
- Tune thread pool sizes based on the nature of the workload - I/O-bound tasks (like API calls) benefit from higher concurrency than CPU-bound tasks.
- Implement connection pooling with appropriate timeout and overflow settings to handle surge loads without exhausting database connections.
- Monitor and log operation timing to identify bottlenecks in the data collection and processing pipeline, focusing optimization efforts on the slowest components.

## Debugging Best Practices

### Systematic Problem-Solving Approach
- Always follow a structured debugging methodology: identify the problem, understand the expected behavior, reproduce consistently, isolate the issue, analyze root cause, implement fix, and verify solution.
- When dealing with data processing issues, prioritize understanding the data flow from source to final storage before attempting fixes.
- Create dedicated debug scripts in the appropriate test directories to isolate and reproduce issues in a controlled environment.
- When debugging data collectors, verify API call parameters, response handling, data transformation, and storage steps individually.

### DataFrame Operations Best Practices
- Always use `df.copy()` when creating a new DataFrame from an existing one to avoid unintended reference issues.
- For numeric data in financial applications, use `Decimal` objects rather than floats to preserve precision.
- When creating a new DataFrame, always ensure it contains all required columns and appropriate defaults for missing values.
- When comparing Decimal values in tests, convert to float or use approximate equality checks to avoid decimal precision comparison issues.

### Code Structure and Data Flow
- Understand the full method call chain before modifying any single function - changes in one function can have ripple effects.
- Pay special attention to data type conversions between API responses, DataFrame processing, and database models.
- Place test files in the proper directory structure that mirrors the source code organization.
- Create debug scripts that provide detailed logging of internal state at critical processing points.

# Scratchpad

